{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "import pandas as pd \n",
    "import pandas_datareader.data as web\n",
    "from pandas import Series, DataFrame\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/new_osemn.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing real time stock data via pandas_datareader.data as web, numerous features, the frequency of this data is daily by default, the date is automatically a pandas datetime obj and is already placed on the x axis so there's no need to perform a pivot method.\n",
    "[pandas-datareader](https://pandas-datareader.readthedocs.io/en/latest/remote_data.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data via web with this implementation there's no need to change date to datetime obj or perform a pivot  \n",
    "# slicing(subsetting) just change the start and end parameters below:\n",
    "start = dt.datetime(2017, 1, 1)\n",
    "end = dt.datetime(2019, 1, 1)\n",
    "\n",
    "SNP = web.DataReader('^GSPC', 'yahoo', start, end).sort_index(ascending=False) # index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import json\n",
    "\n",
    "tweet = open('data/condensed_2017.json')\n",
    "tweets = open('data/condensed_2018.json')\n",
    "\n",
    "tweet17 = json.load(tweet) # 2017 tweets\n",
    "tweet18 = json.load(tweets) # 2018 tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrub:\n",
    "###### S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNP.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the S&P 500 Index\n",
    "SNP.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only interested in the Closing price of the trading day\n",
    "SNP.drop(['High', 'Low', 'Open', 'Adj Close', 'Volume'], axis= 1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail is the most current data\n",
    "# this data is daily frequency\n",
    "print(SNP.head())\n",
    "print(SNP.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNP.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial shape based on the start and end parameters of the imports\n",
    "SNP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual number of trading days limited (of course)\n",
    "from IPython.display import Image\n",
    "Image(filename='images/trading_day.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNP.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no missing values, daily returns\n",
    "SNP.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot\n",
    "SNP.plot(figsize = (16,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot plot\n",
    "# this looks good very similiar to the line plot no outliers just pure data points\n",
    "SNP.plot(figsize=(20,6), style= '.b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas grouper to group values using annual frequency\n",
    "year_groups = SNP['Close'].groupby(pd.Grouper(freq ='A'))\n",
    "\n",
    "# Create a new DataFrame and store yearly values in columns \n",
    "SNP_annual = pd.DataFrame()\n",
    "\n",
    "for yr, group in year_groups:\n",
    "    SNP_annual[yr.year] = group.values\n",
    "    \n",
    "# Plot the yearly groups as subplots\n",
    "SNP_annual.plot(figsize= (13,8), subplots= True, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    "SNP.hist(figsize = (10,6), bins= 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot\n",
    "SNP.plot(kind='kde', figsize = (15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good for spotting outliers, not that there are any in this data\n",
    "SNP_annual.boxplot(figsize = (12,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series heat map\n",
    "year_matrix = SNP_annual.T\n",
    "plt.matshow(year_matrix, interpolation=None, aspect='auto', cmap=plt.cm.Spectral_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stationarity\n",
    "The stationarity was performed much later in this process and the numbers were horrendous, upon gathering some domain knowledge on time series data namely predicting stock, this type of data is susceptible to stationarity over any trending or seasonality issues. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to check for the stationarity of a given time series using rolling stats and DF test\n",
    "# Collect and package the code from previous labs\n",
    "def stationarity_check(TS):\n",
    "    \n",
    "    # Import adfuller\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    \n",
    "    # Calculate rolling statistics\n",
    "    roll_mean = TS.rolling(window=8, center=False).mean()\n",
    "    roll_std = TS.rolling(window=8, center=False).std()\n",
    "    \n",
    "    # Perform the Dickey Fuller Test\n",
    "    dftest = adfuller(TS['Close'])\n",
    "    \n",
    "    # Plot rolling statistics:\n",
    "    fig = plt.figure(figsize=(12,6))\n",
    "    plt.plot(TS, color='blue',label='Original')\n",
    "    plt.plot(roll_mean, color='red', label='Rolling Mean')\n",
    "    plt.plot(roll_std, color='black', label = 'Rolling Std')\n",
    "    plt.legend(loc='best')\n",
    "    plt.title('Rolling Mean & Standard Deviation')\n",
    "    plt.show(block=False)\n",
    "    \n",
    "    # Print Dickey-Fuller test results\n",
    "    print('Results of Dickey-Fuller Test: \\n')\n",
    "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', \n",
    "                                             '#Lags Used', 'Number of Observations Used'])\n",
    "    for key,value in dftest[4].items():\n",
    "        dfoutput['Critical Value (%s)'%key] = value\n",
    "    print(dfoutput)\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check(SNP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a log transform\n",
    "ts_log = np.log(SNP)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(ts_log, color='blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot a square root transform\n",
    "ts_sqrt = np.sqrt(SNP)\n",
    "fig = plt.figure(figsize=(12,6))\n",
    "plt.plot(ts_sqrt, color='blue');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subtracting the rolling mean\n",
    "roll_mean = np.log(SNP).rolling(window=7).mean()\n",
    "fig = plt.figure(figsize=(11,7))\n",
    "plt.plot(np.log(SNP), color='blue', label='Original')\n",
    "plt.plot(roll_mean, color='red', label='Rolling Mean')\n",
    "plt.legend(loc='best')\n",
    "plt.title('Log Transformed Data')\n",
    "plt.show(block=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtract the moving average from the log transformed data\n",
    "data_minus_roll_mean = np.log(SNP) - roll_mean\n",
    "# Print the first 10 rows\n",
    "data_minus_roll_mean.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the missing values\n",
    "data_minus_roll_mean.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(11,7))\n",
    "plt.plot(data_minus_roll_mean, color='blue',label='Close - rolling mean')\n",
    "plt.legend(loc='best')\n",
    "plt.show(block=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stationarity_check(data_minus_roll_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_minus_roll_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNP.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Presidential Tweets\n",
    "##### 2017 Tweets\n",
    "Here I've gather President Trumps tweet history which encompases the month he took office January 20, 2017 in the form of a couple of json files. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tweet17)) # 2605 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json list to pandas Dataframe obj\n",
    "Tweets_17_df = DataFrame(tweet17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns check\n",
    "Tweets_17_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "May end up dropping source as well and just treat every tweet as presidential not \n",
    "just the ones eminating from the iPhone. Will have to clean up the text and use the created_at\n",
    "as a datetime index obj, is_retweet could be used to filter out retweets later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unneeded columns\n",
    "Tweets_17_df.drop(['id_str', 'retweet_count', \n",
    "                   'in_reply_to_user_id_str', 'favorite_count', 'source'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove retweets(RT)\n",
    "Tweets_17_df= Tweets_17_df[Tweets_17_df.is_retweet != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping is_retweet column\n",
    "Tweets_17_df.drop(['is_retweet'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change col created_at to Date\n",
    "Tweets_17_df.rename(columns={'created_at':'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing created_at to datetime index obj\n",
    "# dropping the timestamp\n",
    "Tweets_17_df['Date'] = pd.to_datetime(Tweets_17_df['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_17_df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_17_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets are already in decending order\n",
    "Tweets_17_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2018 Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tweet18)) # 3510 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 tweets to DataFrame obj\n",
    "Tweets_18_df = DataFrame(tweet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unneeded columns\n",
    "Tweets_18_df.drop(['id_str', 'retweet_count', \n",
    "                   'in_reply_to_user_id_str', 'favorite_count', 'source'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove retweets(RT)\n",
    "Tweets_18_df= Tweets_18_df[Tweets_18_df.is_retweet != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping is_retweet column\n",
    "Tweets_18_df.drop(['is_retweet'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change col created_at to Date\n",
    "Tweets_18_df.rename(columns={'created_at':'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing created_at to datetime index obj\n",
    "# dropping the timestamp\n",
    "Tweets_18_df['Date'] = pd.to_datetime(Tweets_18_df['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_18_df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_18_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets are already in decending order\n",
    "Tweets_18_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore:\n",
    "\n",
    "##### Merged Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking Tweets no NaN values and other columns being created with outer merge\n",
    "Tweets = pd.concat([Tweets_17_df, Tweets_18_df], axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweets text cleanup isle 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have an idea for an interactive graph so I'll perform some text cleanup here\n",
    "def txtClean(text):\n",
    "    \"\"\"cleaning text\"\"\"\n",
    "    text = re.sub('@[A-Za-z0â€“9]+', '', text) \n",
    "    text = re.sub('#', '', text) \n",
    "    text = re.sub('https?:\\/\\/\\S+', '', text)\n",
    "    text = text.title() # for graphing time permitting\n",
    "    text = text.lstrip() # suppose to be removing leading space in text\n",
    "    \n",
    "    \n",
    "    return text\n",
    "\n",
    "Tweets['text'] = Tweets['text'].apply(txtClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Twts = Tweets_18_df.head(25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tweets are suppose to be limited to 140 chars but many of these tweets are way over 140 chars\n",
    "probably not a factor in what I'm attempting to achieve in this notebook and actually could ad\n",
    "in sentiment analysis, in the creation of additional features to use in a supervised model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets['length'] = [len(t) for t in Tweets.text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets[Tweets.length > 140].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"TextBlob is a Python (2 and 3) library for processing textual data. It provides a simple API for diving into common natural language processing (NLP) tasks such as part-of-speech tagging, noun phrase extraction, sentiment analysis, classification, translation, and more.\" [TextBlob](https://textblob.readthedocs.io/en/dev/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line doesn't like to be reduced to 80 length for some reason\n",
    "tweet_example = TextBlob('the democrats have been told and fully understand that there can be no daca without the desperately needed wall at the southern border and an end to the horrible chain migration ridiculous lottery system of immigration etc we must protect our country at all cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob seems to have manipulated the text enough to just obtain sentiment scores without additional steps\n",
    "tweet_example.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_example.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how I will determine if a tweet is positive or negative\n",
    "with multiple tweets in a given day I tally the sentiment amoung them together and just\n",
    "take the average, much like a daily presidential sentiment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_example.sentiment.polarity\n",
    "# on a scale of 1(pos) and -1(neg)\n",
    "-0.504166666666667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "polarity - how positive or negative a word is -1 very neg, +1 very pos <br> \n",
    "subjectivity - how opinionated a word is 0 fact, +1 very much an opinion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob test\n",
    "# values are identical when lowercase and all punctuations removed.\n",
    "TextBlob('the democrats have been told and fully understand that there can be no daca without the desperately needed wall at the southern border and an end to the horrible chain migration ridiculous lottery system of immigration etc we must protect our country at all cost').sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sentiment | Polarity 2017 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis on 2017 dataframe\n",
    "polarity = lambda x: TextBlob(x).sentiment.polarity\n",
    "subjectivity = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "Tweets['polarity'] = Tweets['text'].apply(polarity) \n",
    "Tweets['subjectivity'] = Tweets['text'].apply(subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with multiple tweets in a single date\n",
    "Tweet_analysis = Tweets.groupby('Date')['polarity', 'subjectivity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet_analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging sentiment data with SNP data\n",
    "analysis_SNP_df = Tweet_analysis.merge(data_minus_roll_mean, how='right', left_index= True, right_index=True)\n",
    "analysis_SNP_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_SNP_df.dropna(axis= 0, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_SNP_df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model:\n",
    "##### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import preprocessing, utils\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from math import sqrt\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_SNP_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale both target and features\n",
    "scaler = StandardScaler()\n",
    "# analysis_SNP_df = scaler.fit_transform(analysis_SNP_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_SNP_df = pd.DataFrame(analysis_SNP_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_SNP_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the outcome and predictor variables\n",
    "y = analysis_SNP_df['Close']\n",
    "X = analysis_SNP_df.drop('Close', axis=1) \n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size= 0.20, random_state=42) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate and fit a RandomForestRegressor\n",
    "# utilizing hyperparameters from exhaustive GridSearchCV\n",
    "rfr = RandomForestRegressor(criterion = 'mse', \n",
    "                            max_depth = 5, \n",
    "                            max_features = 'log2', \n",
    "                            n_estimators = 100, \n",
    "                            n_jobs = -1) \n",
    "rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "rfr.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subjectivity seems like a slightly better feature than polarity\n",
    "def plot_feature_importances(model):\n",
    "    n_features = X_train.shape[1]\n",
    "    plt.figure(figsize=(8,8))\n",
    "    plt.barh(range(n_features), model.feature_importances_, align='center') \n",
    "    plt.yticks(np.arange(n_features), X_train.columns.values) \n",
    "    plt.xlabel('Feature importance')\n",
    "    plt.ylabel('Feature')\n",
    "    \n",
    "    \n",
    "plot_feature_importances(rfr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "predictions = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the absolute errors\n",
    "errors = abs(predictions - y_test)\n",
    "\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Average model error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (errors / y_test)\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# root mean squared error\n",
    "rsme = math.sqrt(mean_squared_error(y_test, predictions))\n",
    "rsme"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # GridSearchCV\n",
    "# gsc = GridSearchCV(estimator = RandomForestRegressor(),\n",
    "# param_grid = {\n",
    "#     'criterion': ('mse', 'mae'),\n",
    "#     'max_depth': range(3, 7), \n",
    "#     'max_features': ('auto', 'sqrt','log2'), \n",
    "#     'n_estimators': (10 , 50, 100, 1000),\n",
    "#     'n_jobs': (None, -1),\n",
    "#     }, cv = 5, return_train_score= False)\n",
    "\n",
    "# grid_result = gsc.fit(X, y)\n",
    "# best_params = grid_result.best_params_\n",
    "\n",
    "# rfr = RandomForestRegressor(criterion = best_params['criterion'],\n",
    "#                            max_depth = best_params['max_depth'],\n",
    "#                            max_features = best_params['max_features'],\n",
    "#                            n_estimators = best_params['n_estimators'],\n",
    "#                            n_jobs = best_params['n_jobs'], \n",
    "#                             random_state= False, verbose= False)\n",
    "\n",
    "\n",
    "# gsc.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # results from GridSearchCV saved in pandas DataFrame obj\n",
    "# df = pd.DataFrame(gsc.cv_results_)\n",
    "# df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filtering dataframe results\n",
    "#df[['param_criterion', 'param_max_depth', 'param_n_estimators', 'params', 'mean_test_score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's me know what methods and attributes are available for this obj\n",
    "#dir(gsc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # best hyperparameters\n",
    "# gsc.best_estimator_\n",
    "\n",
    "# # RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n",
    "# #                       max_depth=5, max_features='log2', max_leaf_nodes=None,\n",
    "# #                       max_samples=None, min_impurity_decrease=0.0,\n",
    "# #                       min_impurity_split=None, min_samples_leaf=1,\n",
    "# #                       min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "# #                       n_estimators=100, n_jobs=-1, oob_score=False,\n",
    "# #                       random_state=None, verbose=0, warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # best score\n",
    "# gsc.best_score_\n",
    "\n",
    "# #-14.72446175347468"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # best parameters\n",
    "# gsc.best_params_\n",
    "\n",
    "# # {'criterion': 'mse',\n",
    "# #  'max_depth': 5,\n",
    "# #  'max_features': 'log2',\n",
    "# #  'n_estimators': 100,\n",
    "# #  'n_jobs': -1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweet Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join([tweet for tweet in Twts['text']])\n",
    "wordCloud = WordCloud(width=800, height=600, random_state= 21, max_font_size= 120).generate(words)\n",
    "\n",
    "plt.imshow(wordCloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpret:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
