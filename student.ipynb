{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial imports\n",
    "import pandas as pd \n",
    "import pandas_datareader.data as web\n",
    "from pandas import Series, DataFrame\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import re\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from warnings import filterwarnings\n",
    "filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Importing real time stock data via pandas_datareader.data as web\n",
    ">[pandas-datareader](https://pandas-datareader.readthedocs.io/en/latest/remote_data.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data via web with this implementation there's no need to change date to datetime obj or perform a pivot  \n",
    "# slicing(subsetting) just change the start and end parameters below:\n",
    "start = dt.datetime(2017, 1, 1)\n",
    "end = dt.datetime(2019, 1, 1)\n",
    "\n",
    "SNP = web.DataReader('^GSPC', 'yahoo', start, end).sort_index(ascending=False) # index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### S&P 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the S&P 500 Index\n",
    "SNP.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only interested in the Closing price of the trading day\n",
    "SNP.drop(['High', 'Low', 'Open', 'Adj Close', 'Volume'], axis= 1, inplace=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tail is the most current data\n",
    "# this data is daily frequency\n",
    "print(SNP.head())\n",
    "print(SNP.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNP.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial shape based on the start and end parameters of the imports\n",
    "SNP.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annual number of trading days limited (of course)\n",
    "from IPython.display import Image\n",
    "Image(filename='images/trading_day.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SNP.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no missing values, daily returns\n",
    "SNP.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# line plot\n",
    "SNP.plot(figsize = (16,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dot plot\n",
    "# this looks good very similiar to the line plot no outliers just pure data points\n",
    "SNP.plot(figsize=(20,6), style= '.b');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use pandas grouper to group values using annual frequency\n",
    "year_groups = SNP['Close'].groupby(pd.Grouper(freq ='A'))\n",
    "\n",
    "# Create a new DataFrame and store yearly values in columns \n",
    "SNP_annual = pd.DataFrame()\n",
    "\n",
    "for yr, group in year_groups:\n",
    "    SNP_annual[yr.year] = group.values\n",
    "    \n",
    "# Plot the yearly groups as subplots\n",
    "SNP_annual.plot(figsize= (13,8), subplots= True, legend=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# histogram\n",
    "SNP.hist(figsize = (10,6), bins= 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot\n",
    "SNP.plot(kind='kde', figsize = (15,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# good for spotting outliers, not that there are any in this data\n",
    "SNP_annual.boxplot(figsize = (12,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time series heat map\n",
    "year_matrix = SNP_annual.T\n",
    "plt.matshow(year_matrix, interpolation=None, aspect='auto', cmap=plt.cm.Spectral_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Stationarity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Presidential Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import json\n",
    "\n",
    "tweet = open('data/condensed_2017.json')\n",
    "tweets = open('data/condensed_2018.json')\n",
    "\n",
    "tweet17 = json.load(tweet) # 2017 tweets\n",
    "tweet18 = json.load(tweets) # 2018 tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2017 Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tweet17)) # 2605 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# json list to pandas Dataframe obj\n",
    "Tweets_17_df = DataFrame(tweet17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns check\n",
    "Tweets_17_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">May end up dropping source as well and just treat every tweet as presidential not \n",
    ">just the ones eminating from the iPhone. Will have to clean up the text and use the created_at\n",
    ">as a datetime index obj, is_retweet could be used to filter out retweets later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unneeded columns\n",
    "Tweets_17_df.drop(['id_str', 'retweet_count', \n",
    "                   'in_reply_to_user_id_str', 'favorite_count', 'source'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove retweets(RT)\n",
    "Tweets_17_df= Tweets_17_df[Tweets_17_df.is_retweet != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping is_retweet column\n",
    "Tweets_17_df.drop(['is_retweet'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change col created_at to Date\n",
    "Tweets_17_df.rename(columns={'created_at':'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing created_at to datetime index obj\n",
    "# dropping the timestamp\n",
    "Tweets_17_df['Date'] = pd.to_datetime(Tweets_17_df['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_17_df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_17_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets are already in decending order\n",
    "Tweets_17_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2018 Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(tweet18)) # 3510 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2019 tweets to DataFrame obj\n",
    "Tweets_18_df = DataFrame(tweet18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping unneeded columns\n",
    "Tweets_18_df.drop(['id_str', 'retweet_count', \n",
    "                   'in_reply_to_user_id_str', 'favorite_count', 'source'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove retweets(RT)\n",
    "Tweets_18_df= Tweets_18_df[Tweets_18_df.is_retweet != True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping is_retweet column\n",
    "Tweets_18_df.drop(['is_retweet'], axis= 1, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change col created_at to Date\n",
    "Tweets_18_df.rename(columns={'created_at':'Date'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# changing created_at to datetime index obj\n",
    "# dropping the timestamp\n",
    "Tweets_18_df['Date'] = pd.to_datetime(Tweets_18_df['Date']).dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_18_df.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets_18_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tweets are already in decending order\n",
    "Tweets_18_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Merged Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking Tweets no NaN values and other columns being created with outer merge\n",
    "Tweets = pd.concat([Tweets_17_df, Tweets_18_df], axis= 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(Tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweets text cleanup isle 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have an idea for an interactive graph so I'll perform some text cleanup here\n",
    "def txtClean(text):\n",
    "    \"\"\"cleaning text\"\"\"\n",
    "    text = re.sub('@[A-Za-z0–9]+', '', text) \n",
    "    text = re.sub('#', '', text) \n",
    "    text = re.sub('https?:\\/\\/\\S+', '', text)\n",
    "    text = text.title() # for graphing time permitting\n",
    "    text = text.lstrip() # suppose to be removing leading space in text\n",
    "    \n",
    "    \n",
    "    return text\n",
    "\n",
    "Tweets['text'] = Tweets['text'].apply(txtClean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Tweets are suppose to be limited to 140 chars but many of these tweets are way over 140 chars\n",
    ">probably not a factor in what I'm attempting to achieve in this notebook and actually could ad\n",
    ">in sentiment analysis, in the creation of another features to use in a supervised model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets['length'] = [len(t) for t in Tweets.text] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets[Tweets.length > 140].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">TextBlob is the bomb! Is a knowledge based (bag of words) NLP system created by linguist. Will utilize the library to streamline the process. \n",
    ">**TextBlob: Simplified Text Processing** [textblob](https://textblob.readthedocs.io/en/dev/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line doesn't like to be reduced to 80 length for some reason\n",
    "tweet_example = TextBlob('the democrats have been told and fully understand that there can be no daca without the desperately needed wall at the southern border and an end to the horrible chain migration ridiculous lottery system of immigration etc we must protect our country at all cost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob seems to have manipulated the text enough to just obtain sentiment scores without additional steps\n",
    "tweet_example.tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_example.words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how I will determine if a tweet is positive or negative\n",
    "with multiple tweets in a given day I may tally the sentiment amoung them and just\n",
    "take the average, daily_presidential_sentiment.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_example.sentiment.polarity\n",
    "# on a scale of 1(pos) and -1(neg)\n",
    "-0.504166666666667"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">**polarity - how positive or negative a word is -1 very neg, +1 very pos** <br> \n",
    ">**subjectivity - how opinionated a word is 0 fact, +1 very much an opinion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TextBlob test\n",
    "# values are identical when lowercase and all punctuations removed.\n",
    "TextBlob('the democrats have been told and fully understand that there can be no daca without the desperately needed wall at the southern border and an end to the horrible chain migration ridiculous lottery system of immigration etc we must protect our country at all cost').sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Sentiment | Polarity 2017 tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment analysis on 2017 dataframe\n",
    "polarity = lambda x: TextBlob(x).sentiment.polarity\n",
    "subjectivity = lambda x: TextBlob(x).sentiment.subjectivity\n",
    "\n",
    "Tweets['polarity'] = Tweets['text'].apply(polarity) \n",
    "Tweets['subjectivity'] = Tweets['text'].apply(subjectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dealing with multiple tweets in a single date\n",
    "Tweet_analysis = Tweets.groupby('Date')['polarity', 'subjectivity'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet_analysis.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tweet_analysis.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merging sentiment data with SNP data\n",
    "analysis_SNP_df = Tweet_analysis.merge(SNP, how='right', left_index= True, right_index=True)\n",
    "analysis_SNP_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_SNP_df.dropna(axis= 0, inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis_SNP_df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling Close column\n",
    "scaler = StandardScaler()\n",
    "scaled_close = scaler.fit_transform(analysis_SNP_df.Close.values.reshape(-1, 1))\n",
    "scaled_close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scaled_close)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import BaggingClassifier, RandomForestRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import preprocessing, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the outcome and predictor variables\n",
    "target = analysis_SNP_df['Close']\n",
    "data = analysis_SNP_df.drop(\"Close\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split\n",
    "data_train, data_test, target_train, target_test = train_test_split(data, target, \n",
    "                                                                    test_size = 0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ValueError: Unknown label type: 'continuous'\n",
    "# this is continous not classification \n",
    "# forest = RandomForestClassifier(n_estimators=100, max_depth= 5)\n",
    "# forest.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest = RandomForestRegressor()\n",
    "# forest.fit(data_train, target_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate RandomForestRegressor\n",
    "rf_Reg = RandomForestRegressor()\n",
    "# mean_rf_reg_score = cross_val_score(rf_Reg, data_train, target_train, cv=3)\n",
    "# print(f\"Mean Cross Validation Score for Random Forest Classifier: {mean_rf_reg_score :.2%}\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators': [10, 30, 50, 100],# trees in the forest\n",
    "    'criterion': ['mae', 'mse'], # mean abs error, mean square error, \n",
    "    'max_depth': [None], # None nodes are expanded until all leaves are pure ...\n",
    "    'random_state': 42,\n",
    "    'max_features': ['auto', 'sqrt', 'log2'],\n",
    "    'n_jobs': -1,# utilize all processors\n",
    "    'max_leaf_nodes': [None, 5, 10, 15, 25],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate GridSearchCV\n",
    "# gs_CV = GridSearchCV(rf_Reg, rf_param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_grid_search = GridSearchCV(rf_Reg, rf_param_grid, cv=3)\n",
    "rf_grid_search.fit(data_train, target_train)\n",
    "\n",
    "print(f\"Training Accuracy: {rf_grid_search.best_score_ :.2%}\")\n",
    "print(\"\")\n",
    "print(f\"Optimal Parameters: {rf_grid_search.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Tweet Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ' '.join([tweet for tweet in Tweets['text']])\n",
    "wordCloud = WordCloud(width=800, height=600, random_state= 21, max_font_size= 120).generate(words)\n",
    "\n",
    "plt.imshow(wordCloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.8px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
